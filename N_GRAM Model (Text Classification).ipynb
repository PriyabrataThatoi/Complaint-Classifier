{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text classification n-gram(s) model \n",
    "\n",
    "![ngram](https://user-images.githubusercontent.com/54467567/68990210-27e76400-0816-11ea-9bdb-5057291a9f91.jpg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Thatoi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Thatoi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import tarfile\n",
    "import os\n",
    "import pyprind\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "########### TOKENIZATIN ############\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.util import ngrams\n",
    "\n",
    "########### STEMMING ###############\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "\n",
    "########### STOPWORDS ##############\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "########### LEMMATIZATION ##########\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "nltk.download('wordnet')\n",
    "\n",
    "########### LDA ####################\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "########### TEST & TRAIN SPLIT #####\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "########### LOGISTIC REGRESSION #####\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "os.chdir(r\"C:\\important_files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tar.gz file extraction\n",
    "with tarfile.open('aclImdb_v1.tar.gz','r:gz') as tar :\n",
    "    tar.extractall()\n",
    "\n",
    "# combining files into a dataframe\n",
    "labels = {'pos' :1 ,\n",
    "          'neg' :0}\n",
    "\n",
    "pbar = pyprind.ProgBar(50000)\n",
    "df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:03:01\n"
     ]
    }
   ],
   "source": [
    "# creating an appended file\n",
    "basepath = 'aclImdb'\n",
    "for s in ('test','train'):\n",
    "    for l in ('pos','neg'):\n",
    "        path = os.path.join(basepath,s,l)\n",
    "        for file in sorted(os.listdir(path)):\n",
    "            with open(os.path.join(path, file),\n",
    "                      'r',encoding='utf-8') as infile:\n",
    "                txt = infile.read()\n",
    "            df =df.append([[txt,labels[l]]], ignore_index=True)\n",
    "            pbar.update()\n",
    "df.columns=['reviews','sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I went and saw this movie last night after bei...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Actor turned director Bill Paxton follows up h...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>As a recreational golfer with some knowledge o...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I saw this film in a sneak preview, and it is ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bill Paxton has taken the true story of the 19...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             reviews  sentiment\n",
       "0  I went and saw this movie last night after bei...          1\n",
       "1  Actor turned director Bill Paxton follows up h...          1\n",
       "2  As a recreational golfer with some knowledge o...          1\n",
       "3  I saw this film in a sneak preview, and it is ...          1\n",
       "4  Bill Paxton has taken the true story of the 19...          1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 2)\n",
      "                                                 reviews  sentiment\n",
      "11841  In 1974, the teenager Martha Moxley (Maggie Gr...          1\n",
      "19602  OK... so... I really like Kris Kristofferson a...          0\n",
      "45519  ***SPOILER*** Do not read this, if you think a...          0\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "df = df.reindex(np.random.permutation(df.index))\n",
    "print(df.shape)\n",
    "print(df.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process of performing sentiment analysis :\n",
    "![model](https://user-images.githubusercontent.com/54467567/68970713-89271d00-07ad-11ea-87d2-e5d55344a31c.PNG)\n",
    "\n",
    "reference : https://www.datacamp.com/community/tutorials/text-analytics-beginners-nltk\n",
    "## Cleaning dataset {html tags, emoticons & non-characters}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Whether one views him as a gallant cavalier of the plains or a glory hunting egomaniac, debates about the life and military career of George Armstrong Custer continue down to the present day. They Died With Their Boots On presents certain facts of the Custer story and has taken liberty with others.<br /><br />He did in fact graduate at the bottom of his class at West Point and got this overnight promotion on the battlefield to Brigadier General. His record leading the Michigan Regiment under his command was one of brilliance.<br /><br />It was also true that his marriage to Libby Bacon was one of the great love matches of the 19th century. Libby and George were married for 12 years until The Little Big Horn. What's not known to today's audience is that Libby survived until 1933. During that time she was the custodian of the Custer legend. By dint of her own iron will and force of personality her late husband became a hero because she would not allow him to be remembered in any other way.<br /><br />I think Raoul Walsh and Warner Brothers missed a good opportunity to have the Custer career told in flashback. Olivia DeHavilland should have been made up the way Jeanette MacDonald was in Maytime, and be telling the story of her husband and her marriage from the point of view of nostalgia and remembrance. Even then the cracks in the Custer legend were appearing, but if done from Libby's point of view, they could be understood and forgiven.<br /><br />Sydney Greenstreet gave a fine performance as General Winfield Scott. The only problem was that Scott had nothing whatsoever to do with Custer, he was retired and replaced by George B. McClellan in late 1861 while Custer was still at West Point. I'm not sure they ever met. But Greenstreet does a good characterization of the ponderous and powerful Winfield Scott. A nice Mexican War story should have been what they gave Greenstreet instead for his very accurate portrayal of old Fuss and Feathers.<br /><br />The film though is carried by one of the great romantic teams of cinema, Errol Flynn and Olivia DeHavilland. This was the last of eight films they did together. The last scene they ever did for the cameras was Libby's farewell to George as he leaves to join his regiment for what will prove to be his last campaign. Both their performances, Olivia's especially, was a high point in their careers at Warner Brothers. We know through history that Custer is riding to his doom, that and the fact that this was their last screen teaming give this scene such a special poignancy. If your eyes don't moisten you are made of marble. <br /><br />As history They Died With Their Boots On leaves a lot to be desired. As western adventure that successfully mixes romance with the action, you can't beat this film at all.\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[50,'reviews']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessor(text):\n",
    "    # replaing HTML tags\n",
    "    text = re.sub('<[^>]*>', '', text) \n",
    "    # storing emoticons\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', \n",
    "                           text)\n",
    "    # remove non-characters, lowercasing and adding  emoticons\n",
    "    text = (re.sub('[\\W]+', ' ', text.lower()) + \n",
    "            ' '.join(emoticons).replace('-', '')) # removing nose in emoticons\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'whether one views him as a gallant cavalier of the plains or a glory hunting egomaniac debates about the life and military career of george armstrong custer continue down to the present day they died with their boots on presents certain facts of the custer story and has taken liberty with others he did in fact graduate at the bottom of his class at west point and got this overnight promotion on the battlefield to brigadier general his record leading the michigan regiment under his command was one of brilliance it was also true that his marriage to libby bacon was one of the great love matches of the 19th century libby and george were married for 12 years until the little big horn what s not known to today s audience is that libby survived until 1933 during that time she was the custodian of the custer legend by dint of her own iron will and force of personality her late husband became a hero because she would not allow him to be remembered in any other way i think raoul walsh and warner brothers missed a good opportunity to have the custer career told in flashback olivia dehavilland should have been made up the way jeanette macdonald was in maytime and be telling the story of her husband and her marriage from the point of view of nostalgia and remembrance even then the cracks in the custer legend were appearing but if done from libby s point of view they could be understood and forgiven sydney greenstreet gave a fine performance as general winfield scott the only problem was that scott had nothing whatsoever to do with custer he was retired and replaced by george b mcclellan in late 1861 while custer was still at west point i m not sure they ever met but greenstreet does a good characterization of the ponderous and powerful winfield scott a nice mexican war story should have been what they gave greenstreet instead for his very accurate portrayal of old fuss and feathers the film though is carried by one of the great romantic teams of cinema errol flynn and olivia dehavilland this was the last of eight films they did together the last scene they ever did for the cameras was libby s farewell to george as he leaves to join his regiment for what will prove to be his last campaign both their performances olivia s especially was a high point in their careers at warner brothers we know through history that custer is riding to his doom that and the fact that this was their last screen teaming give this scene such a special poignancy if your eyes don t moisten you are made of marble as history they died with their boots on leaves a lot to be desired as western adventure that successfully mixes romance with the action you can t beat this film at all '"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#example\n",
    "preprocessor(df.loc[50,'reviews'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['reviews'] = df['reviews'].apply(preprocessor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK Ngrams, Stemming  & Stop words removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization :\n",
    "Tokenization is the process by which big quantity of text is divided into smaller parts called tokens.\n",
    "Natural language processing is used for building applications such as Text classification, intelligent chatbot,\n",
    "sentimental analysis, language translation, etc. It becomes vital to understand the pattern in the text to achieve\n",
    "the above-stated purpose. These tokens are very useful for finding such patterns as well as is considered as a \n",
    "base step for stemming and lemmatization.\n",
    "\n",
    "##### multigram tokens\n",
    "When performing machine learning tasks related to natural language processing, we usually need to generate n-grams from input sentences. For example, in text classification tasks, in addition to using each individual token found in the corpus, we may want to add bi-grams or tri-grams as features to represent our documents. bi-grams or tri-grams adds more predictive power to the model as it includes partial sentence with the subject and verb unlike the uni-gram model that contain only a single word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ngrams(data, num):\n",
    "    '''\n",
    "    definition to create n-grams of tokens \n",
    "    from the sentences using nltk.tokenize\n",
    "    method\n",
    "    '''\n",
    "    n_grams = ngrams(nltk.word_tokenize(data), num)\n",
    "    return [ ' '.join(grams) for grams in n_grams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating 1,2 & 3 grams tokens \n",
    "df['1gram_tokens'] = ''\n",
    "df['2gram_tokens'] = ''\n",
    "df['3gram_tokens'] = ''\n",
    "\n",
    "for i in df.index:\n",
    "    df.at[i,'1gram_tokens'] = extract_ngrams(df.at[i,'reviews'],1)\n",
    "    df.at[i,'2gram_tokens'] = extract_ngrams(df.at[i,'reviews'],2)\n",
    "    df.at[i,'3gram_tokens'] = extract_ngrams(df.at[i,'reviews'],3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews</th>\n",
       "      <th>1gram_tokens</th>\n",
       "      <th>2gram_tokens</th>\n",
       "      <th>3gram_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11841</th>\n",
       "      <td>in 1974 the teenager martha moxley maggie grac...</td>\n",
       "      <td>[in, 1974, the, teenag, martha, moxley, maggi,...</td>\n",
       "      <td>[in 1974, 1974 the, the teenag, teenager marth...</td>\n",
       "      <td>[in 1974 th, 1974 the teenag, the teenager mar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19602</th>\n",
       "      <td>ok so i really like kris kristofferson and his...</td>\n",
       "      <td>[ok, so, i, realli, like, kri, kristofferson, ...</td>\n",
       "      <td>[ok so, so i, i real, really lik, like kri, kr...</td>\n",
       "      <td>[ok so i, so i r, i really lik, really like kr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45519</th>\n",
       "      <td>spoiler do not read this if you think about w...</td>\n",
       "      <td>[spoiler, do, not, read, thi, if, you, think, ...</td>\n",
       "      <td>[spoiler do, do not, not read, read thi, this ...</td>\n",
       "      <td>[spoiler do not, do not read, not read thi, re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25747</th>\n",
       "      <td>hi for all the people who have seen this wonde...</td>\n",
       "      <td>[hi, for, all, the, peopl, who, have, seen, th...</td>\n",
       "      <td>[hi for, for al, all th, the peopl, people who...</td>\n",
       "      <td>[hi for al, for all th, all the peopl, the peo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42642</th>\n",
       "      <td>i recently bought the dvd forgetting just how ...</td>\n",
       "      <td>[i, recent, bought, the, dvd, forget, just, ho...</td>\n",
       "      <td>[i rec, recently bought, bought th, the dvd, d...</td>\n",
       "      <td>[i recently bought, recently bought th, bought...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 reviews  \\\n",
       "11841  in 1974 the teenager martha moxley maggie grac...   \n",
       "19602  ok so i really like kris kristofferson and his...   \n",
       "45519   spoiler do not read this if you think about w...   \n",
       "25747  hi for all the people who have seen this wonde...   \n",
       "42642  i recently bought the dvd forgetting just how ...   \n",
       "\n",
       "                                            1gram_tokens  \\\n",
       "11841  [in, 1974, the, teenag, martha, moxley, maggi,...   \n",
       "19602  [ok, so, i, realli, like, kri, kristofferson, ...   \n",
       "45519  [spoiler, do, not, read, thi, if, you, think, ...   \n",
       "25747  [hi, for, all, the, peopl, who, have, seen, th...   \n",
       "42642  [i, recent, bought, the, dvd, forget, just, ho...   \n",
       "\n",
       "                                            2gram_tokens  \\\n",
       "11841  [in 1974, 1974 the, the teenag, teenager marth...   \n",
       "19602  [ok so, so i, i real, really lik, like kri, kr...   \n",
       "45519  [spoiler do, do not, not read, read thi, this ...   \n",
       "25747  [hi for, for al, all th, the peopl, people who...   \n",
       "42642  [i rec, recently bought, bought th, the dvd, d...   \n",
       "\n",
       "                                            3gram_tokens  \n",
       "11841  [in 1974 th, 1974 the teenag, the teenager mar...  \n",
       "19602  [ok so i, so i r, i really lik, really like kr...  \n",
       "45519  [spoiler do not, do not read, not read thi, re...  \n",
       "25747  [hi for al, for all th, all the peopl, the peo...  \n",
       "42642  [i recently bought, recently bought th, bought...  "
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['reviews','1gram_tokens','2gram_tokens','3gram_tokens']].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemming :\n",
    "Stemming algorithms work by cutting off the end or the beginning of the word, taking into account a list of common prefixes and suffixes that can be found in an inflected word. This indiscriminate cutting can be successful in some occasions, but not always, and that is why we affirm that this approach presents some limitations. Below we illustrate the method with examples in both English and Spanish (It requires knowledge of suffix and prefix)\n",
    "\n",
    "![stemming](https://user-images.githubusercontent.com/54467567/68964605-6aba2500-079f-11ea-9144-6e4c5b43cc79.PNG)\n",
    "\n",
    "reference : https://blog.bitext.com/what-is-the-difference-between-stemming-and-lemmatization/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming(text) :\n",
    "    '''\n",
    "    creating definition to generate\n",
    "    1-gram tokens and stem those to \n",
    "    the root words using porter stemming\n",
    "    algorithm that removes morphological \n",
    "    endings from the words\n",
    "    '''\n",
    "    return [porter.stem(word) for word in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['1gram_tokens'] = df['1gram_tokens'].apply(stemming)\n",
    "df['2gram_tokens'] = df['2gram_tokens'].apply(stemming)\n",
    "df['3gram_tokens'] = df['3gram_tokens'].apply(stemming)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STOP word removal :\n",
    "When working with text mining applications, we often hear of the term “stop words” or “stop word list” or even “stop list”. Stop words are just a set of commonly used words in any language. Stop words are commonly eliminated from many text processing applications because these words can be distracting, non-informative (or non-discriminative) and are additional memory overhead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\priyabrata.thatoi\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]     .\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "stop = stopwords.words('english')\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    '''\n",
    "    creating definition to remove\n",
    "    stopwords from tokenized \n",
    "    content in the text field\n",
    "    '''\n",
    "    list_words= []\n",
    "    for w in text: \n",
    "        if w not in stop:\n",
    "            list_words.append(w)\n",
    "    return (list_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['1gram_tokens'] = df['1gram_tokens'].apply(remove_stopwords)\n",
    "df['2gram_tokens'] = df['2gram_tokens'].apply(remove_stopwords)\n",
    "df['3gram_tokens'] = df['3gram_tokens'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews</th>\n",
       "      <th>1gram_tokens</th>\n",
       "      <th>2gram_tokens</th>\n",
       "      <th>3gram_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11841</th>\n",
       "      <td>in 1974 the teenager martha moxley maggie grac...</td>\n",
       "      <td>[1974, teenag, martha, moxley, maggi, grace, m...</td>\n",
       "      <td>[in 1974, 1974 the, the teenag, teenager marth...</td>\n",
       "      <td>[in 1974 th, 1974 the teenag, the teenager mar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19602</th>\n",
       "      <td>ok so i really like kris kristofferson and his...</td>\n",
       "      <td>[ok, realli, like, kri, kristofferson, hi, usu...</td>\n",
       "      <td>[ok so, so i, i real, really lik, like kri, kr...</td>\n",
       "      <td>[ok so i, so i r, i really lik, really like kr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45519</th>\n",
       "      <td>spoiler do not read this if you think about w...</td>\n",
       "      <td>[spoiler, read, thi, think, watch, movi, altho...</td>\n",
       "      <td>[spoiler do, do not, not read, read thi, this ...</td>\n",
       "      <td>[spoiler do not, do not read, not read thi, re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25747</th>\n",
       "      <td>hi for all the people who have seen this wonde...</td>\n",
       "      <td>[hi, peopl, seen, thi, wonder, movi, im, sure,...</td>\n",
       "      <td>[hi for, for al, all th, the peopl, people who...</td>\n",
       "      <td>[hi for al, for all th, all the peopl, the peo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42642</th>\n",
       "      <td>i recently bought the dvd forgetting just how ...</td>\n",
       "      <td>[recent, bought, dvd, forget, much, hate, movi...</td>\n",
       "      <td>[i rec, recently bought, bought th, the dvd, d...</td>\n",
       "      <td>[i recently bought, recently bought th, bought...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 reviews  \\\n",
       "11841  in 1974 the teenager martha moxley maggie grac...   \n",
       "19602  ok so i really like kris kristofferson and his...   \n",
       "45519   spoiler do not read this if you think about w...   \n",
       "25747  hi for all the people who have seen this wonde...   \n",
       "42642  i recently bought the dvd forgetting just how ...   \n",
       "\n",
       "                                            1gram_tokens  \\\n",
       "11841  [1974, teenag, martha, moxley, maggi, grace, m...   \n",
       "19602  [ok, realli, like, kri, kristofferson, hi, usu...   \n",
       "45519  [spoiler, read, thi, think, watch, movi, altho...   \n",
       "25747  [hi, peopl, seen, thi, wonder, movi, im, sure,...   \n",
       "42642  [recent, bought, dvd, forget, much, hate, movi...   \n",
       "\n",
       "                                            2gram_tokens  \\\n",
       "11841  [in 1974, 1974 the, the teenag, teenager marth...   \n",
       "19602  [ok so, so i, i real, really lik, like kri, kr...   \n",
       "45519  [spoiler do, do not, not read, read thi, this ...   \n",
       "25747  [hi for, for al, all th, the peopl, people who...   \n",
       "42642  [i rec, recently bought, bought th, the dvd, d...   \n",
       "\n",
       "                                            3gram_tokens  \n",
       "11841  [in 1974 th, 1974 the teenag, the teenager mar...  \n",
       "19602  [ok so i, so i r, i really lik, really like kr...  \n",
       "45519  [spoiler do not, do not read, not read thi, re...  \n",
       "25747  [hi for al, for all th, all the peopl, the peo...  \n",
       "42642  [i recently bought, recently bought th, bought...  "
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BEFORE APPLYING STOPWORD REMOVAL\n",
    "df[['reviews','1gram_tokens','2gram_tokens','3gram_tokens']].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatization : \n",
    "Lemmatization, on the other hand, takes into consideration the morphological analysis of the words. To do so, it is necessary to have detailed dictionaries which the algorithm can look through to link the form back to its lemma. See how it works with the same example words.(It requires the complete dictionary of the language)\n",
    "![lemma](https://user-images.githubusercontent.com/54467567/68964888-13688480-07a0-11ea-8745-578d07bd186f.PNG)\n",
    "\n",
    "reference : https://blog.bitext.com/what-is-the-difference-between-stemming-and-lemmatization/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def lemmatize_word(text):\n",
    "    '''\n",
    "    creating definition to convert words \n",
    "    into lemmas (root words)\n",
    "    '''\n",
    "    return [lemmatizer.lemmatize(word) for word in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['1gram_tokens'] = df['1gram_tokens'].apply(lemmatize_word)\n",
    "df['2gram_tokens'] = df['2gram_tokens'].apply(lemmatize_word)\n",
    "df['3gram_tokens'] = df['3gram_tokens'].apply(lemmatize_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SKLearn Ngrams, Tf-Idf vector & Text Topics using Latent Dirichlet Allocation\n",
    "SKlearn has a direct package(CountVectorizer) which generates n-grams from the text. It produces the bag-of-word matrixs based on the n-grams. In this method, it automatically lower cases the text and removes the stop words. therefore explicitly performing these stems are not required. Latent Dirichlet Allocation is a probabilistic method that looks for group of words frequently occuring in the documents. The input of the LDA is the bag-of-words model matrix. It decomposes bag-of-word matrix into a document-to-topic matrix and a word-to-topic matrix.\n",
    "\n",
    "### TF-IDF\n",
    "\n",
    "Term-Frequency and Inverse Document Frequency is the process of creating numeric values for each of the text terms and assigning proper weigts as per the relevance of the word in the documents. It is calculated as\n",
    "  \n",
    "\n",
    "$$\\text{tf-idf}(t,d)=\\text{tf (t,d)}\\times \\text{idf}(t,d)$$\n",
    "\n",
    "\n",
    "where $$\\text{tf (t,d)}$$ is frequency distribution of each word in the document. it creates a sparse vector with the index of the word and frequency/count of the word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATING 1,2 & 3 GRAMS WORD FROM TEXT\n",
    "count_1 = TfidfVectorizer(max_df=.1 , max_features = 50, ngram_range=(1,1))\n",
    "bag_1 = count_1.fit_transform(df['reviews'].values)\n",
    "\n",
    "count_2 = TfidfVectorizer(max_df=.1 , max_features = 50, ngram_range=(2,2))\n",
    "bag_2 = count_2.fit_transform(df['reviews'].values)\n",
    "\n",
    "count_3 = TfidfVectorizer(max_df=.1 , max_features = 50, ngram_range=(3,3))\n",
    "bag_3 = count_3.fit_transform(df['reviews'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>action</th>\n",
       "      <th>although</th>\n",
       "      <th>am</th>\n",
       "      <th>anyone</th>\n",
       "      <th>away</th>\n",
       "      <th>believe</th>\n",
       "      <th>book</th>\n",
       "      <th>comedy</th>\n",
       "      <th>course</th>\n",
       "      <th>day</th>\n",
       "      <th>...</th>\n",
       "      <th>script</th>\n",
       "      <th>series</th>\n",
       "      <th>set</th>\n",
       "      <th>since</th>\n",
       "      <th>sure</th>\n",
       "      <th>trying</th>\n",
       "      <th>tv</th>\n",
       "      <th>woman</th>\n",
       "      <th>worst</th>\n",
       "      <th>yet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.308650</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.309077</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.318364</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.525235</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.267626</td>\n",
       "      <td>0.265719</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.269061</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.445067</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.448112</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.708978</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     action  although   am    anyone  away  believe  book    comedy    course  \\\n",
       "0  0.000000  0.000000  0.0  0.308650   0.0      0.0   0.0  0.309077  0.000000   \n",
       "1  0.525235  0.000000  0.0  0.000000   0.0      0.0   0.0  0.000000  0.267626   \n",
       "2  0.000000  0.445067  0.0  0.000000   0.0      0.0   0.0  0.000000  0.448112   \n",
       "3  0.000000  0.000000  0.0  0.708978   0.0      0.0   0.0  0.000000  0.000000   \n",
       "4  0.000000  0.000000  0.0  0.000000   0.0      0.0   0.0  0.000000  0.000000   \n",
       "\n",
       "        day  ...  script  series       set  since  sure    trying   tv  woman  \\\n",
       "0  0.000000  ...     0.0     0.0  0.000000    0.0   0.0  0.318364  0.0    0.0   \n",
       "1  0.265719  ...     0.0     0.0  0.269061    0.0   0.0  0.000000  0.0    0.0   \n",
       "2  0.000000  ...     0.0     0.0  0.000000    0.0   0.0  0.000000  0.0    0.0   \n",
       "3  0.000000  ...     0.0     0.0  0.000000    0.0   0.0  0.000000  0.0    0.0   \n",
       "4  0.000000  ...     0.0     0.0  0.000000    0.0   0.0  0.000000  0.0    0.0   \n",
       "\n",
       "   worst  yet  \n",
       "0    0.0  0.0  \n",
       "1    0.0  0.0  \n",
       "2    0.0  0.0  \n",
       "3    0.0  0.0  \n",
       "4    0.0  0.0  \n",
       "\n",
       "[5 rows x 50 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SAMPLE DATAFRAME CREATED TO DO SENTIMENT MODELING\n",
    "a=pd.DataFrame(bag_1.toarray())\n",
    "a.columns = count_1.get_feature_names()\n",
    "a.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 3 from 1-gram bag of word model by tfidf value : ['goes' 'day' 'yet']\n"
     ]
    }
   ],
   "source": [
    "feature_array = np.array(count_1.get_feature_names())\n",
    "tfidf_sorting = np.argsort(bag_1.toarray()).flatten()[::-1]\n",
    "\n",
    "n = 3\n",
    "top_n = feature_array[tfidf_sorting[:n]]\n",
    "print('Top 3 from 1-gram bag of word model by tfidf value :' , top_n )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>and his</th>\n",
       "      <th>as well</th>\n",
       "      <th>at all</th>\n",
       "      <th>at least</th>\n",
       "      <th>br this</th>\n",
       "      <th>could have</th>\n",
       "      <th>going to</th>\n",
       "      <th>have to</th>\n",
       "      <th>he is</th>\n",
       "      <th>he was</th>\n",
       "      <th>...</th>\n",
       "      <th>to say</th>\n",
       "      <th>trying to</th>\n",
       "      <th>want to</th>\n",
       "      <th>was the</th>\n",
       "      <th>which is</th>\n",
       "      <th>who is</th>\n",
       "      <th>would be</th>\n",
       "      <th>would have</th>\n",
       "      <th>you can</th>\n",
       "      <th>you re</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.444586</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.471144</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.710714</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.530374</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.485885</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.410938</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.409518</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   and his   as well  at all  at least   br this  could have  going to  \\\n",
       "0      0.0  0.444586     0.0       0.0  0.000000    0.000000  0.000000   \n",
       "1      0.0  0.000000     0.0       0.0  0.000000    0.000000  0.710714   \n",
       "2      0.0  0.000000     0.0       0.0  0.000000    0.530374  0.000000   \n",
       "3      0.0  0.410938     0.0       0.0  0.409518    0.000000  0.000000   \n",
       "4      0.0  0.000000     0.0       0.0  0.000000    0.000000  0.000000   \n",
       "\n",
       "   have to     he is    he was  ...  to say  trying to  want to  was the  \\\n",
       "0      0.0  0.000000  0.471144  ...     0.0        0.0      0.0      0.0   \n",
       "1      0.0  0.000000  0.000000  ...     0.0        0.0      0.0      0.0   \n",
       "2      0.0  0.485885  0.000000  ...     0.0        0.0      0.0      0.0   \n",
       "3      0.0  0.000000  0.000000  ...     0.0        0.0      0.0      0.0   \n",
       "4      0.0  0.000000  0.000000  ...     0.0        0.0      0.0      0.0   \n",
       "\n",
       "   which is  who is  would be  would have  you can  you re  \n",
       "0       0.0     0.0       0.0         0.0      0.0     0.0  \n",
       "1       0.0     0.0       0.0         0.0      0.0     0.0  \n",
       "2       0.0     0.0       0.0         0.0      0.0     0.0  \n",
       "3       0.0     0.0       0.0         0.0      0.0     0.0  \n",
       "4       0.0     0.0       0.0         0.0      0.0     0.0  \n",
       "\n",
       "[5 rows x 50 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SAMPLE DATAFRAME CREATED TO DO SENTIMENT MODELING\n",
    "b=pd.DataFrame(bag_2.toarray())\n",
    "b.columns = count_2.get_feature_names()\n",
    "b.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 3 from 2-grams bag of word model by tfidf value : ['to say' 'the whole' 'is one']\n"
     ]
    }
   ],
   "source": [
    "feature_array = np.array(count_2.get_feature_names())\n",
    "tfidf_sorting = np.argsort(bag_2.toarray()).flatten()[::-1]\n",
    "\n",
    "n = 3\n",
    "top_n = feature_array[tfidf_sorting[:n]]\n",
    "print('Top 3 from 2-grams bag of word model by tfidf value :' , top_n )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>as well as</th>\n",
       "      <th>at the end</th>\n",
       "      <th>br br and</th>\n",
       "      <th>br br but</th>\n",
       "      <th>br br if</th>\n",
       "      <th>br br in</th>\n",
       "      <th>br br it</th>\n",
       "      <th>br br there</th>\n",
       "      <th>br br this</th>\n",
       "      <th>could have been</th>\n",
       "      <th>...</th>\n",
       "      <th>the movie was</th>\n",
       "      <th>the rest of</th>\n",
       "      <th>the story is</th>\n",
       "      <th>there is no</th>\n",
       "      <th>this film is</th>\n",
       "      <th>this is one</th>\n",
       "      <th>this is the</th>\n",
       "      <th>this movie is</th>\n",
       "      <th>this movie was</th>\n",
       "      <th>would have been</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.748112</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.656479</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.420694</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.514837</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.541982</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   as well as  at the end  br br and  br br but  br br if  br br in  br br it  \\\n",
       "0         0.0         0.0        0.0        0.0       0.0       0.0       0.0   \n",
       "1         0.0         0.0        0.0        0.0       0.0       0.0       0.0   \n",
       "2         0.0         0.0        0.0        0.0       0.0       0.0       0.0   \n",
       "3         0.0         0.0        0.0        0.0       0.0       0.0       0.0   \n",
       "4         0.0         0.0        0.0        0.0       0.0       0.0       0.0   \n",
       "\n",
       "   br br there  br br this  could have been  ...  the movie was  the rest of  \\\n",
       "0          0.0    0.000000              0.0  ...       0.000000          0.0   \n",
       "1          0.0    0.000000              0.0  ...       0.000000          0.0   \n",
       "2          0.0    0.000000              0.0  ...       0.656479          0.0   \n",
       "3          0.0    0.420694              0.0  ...       0.000000          0.0   \n",
       "4          0.0    0.000000              0.0  ...       0.000000          0.0   \n",
       "\n",
       "   the story is  there is no  this film is  this is one  this is the  \\\n",
       "0      0.000000          0.0           0.0     0.000000          0.0   \n",
       "1      0.000000          0.0           0.0     0.000000          0.0   \n",
       "2      0.000000          0.0           0.0     0.000000          0.0   \n",
       "3      0.514837          0.0           0.0     0.541982          0.0   \n",
       "4      0.000000          0.0           0.0     0.000000          0.0   \n",
       "\n",
       "   this movie is  this movie was  would have been  \n",
       "0            0.0        0.748112              0.0  \n",
       "1            0.0        0.000000              0.0  \n",
       "2            0.0        0.000000              0.0  \n",
       "3            0.0        0.000000              0.0  \n",
       "4            0.0        0.000000              0.0  \n",
       "\n",
       "[5 rows x 50 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SAMPLE DATAFRAME CREATED TO DO SENTIMENT MODELING\n",
    "c=pd.DataFrame(bag_3.toarray())\n",
    "c.columns = count_3.get_feature_names()\n",
    "c.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 3 from 3-grams bag of word model by tfidf value : ['this is one' 'br br there' 'is one of']\n"
     ]
    }
   ],
   "source": [
    "feature_array = np.array(count_3.get_feature_names())\n",
    "tfidf_sorting = np.argsort(bag_3.toarray()).flatten()[::-1]\n",
    "\n",
    "n = 3\n",
    "top_n = feature_array[tfidf_sorting[:n]]\n",
    "print('Top 3 from 3-grams bag of word model by tfidf value :' , top_n )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1:\n",
      "ever seen the worst the book have ever movie and\n",
      "Topic 2:\n",
      "he is and his she is in his who is\n",
      "Topic 3:\n",
      "was the this was he was was very and was\n",
      "Topic 4:\n",
      "the show this show you can they are the characters\n",
      "Topic 5:\n",
      "would have could have there was was the this was\n",
      "Topic 6:\n",
      "he is that he he was they are is that\n",
      "Topic 7:\n",
      "the original the characters into the the world some of\n",
      "Topic 8:\n",
      "the acting special effects the whole the rest at all\n",
      "Topic 9:\n",
      "is very the acting very good is one is great\n",
      "Topic 10:\n",
      "you re you can kind of it not going to\n"
     ]
    }
   ],
   "source": [
    "# EXAMPLE: TEXT TOPICS ON BIGRAM WORDS\n",
    "lda_2 = LatentDirichletAllocation(n_components=10,random_state=123,learning_method='batch')\n",
    "X_topics = lda_2.fit_transform(bag_2)\n",
    "\n",
    "n_top_words = 5\n",
    "feature_names = count.get_feature_names()\n",
    "for topic_idx, topic in enumerate(lda_2.components_) :\n",
    "    print(\"Topic %d:\" % (topic_idx+1))\n",
    "    print(\" \".join([feature_names[i]\n",
    "                   for i in topic.argsort()\\\n",
    "                   [:-n_top_words - 1: -1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling  : Text Classifier - ngram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADDING TARGET VARIABLE\n",
    "a['sentiment'] = df['sentiment']\n",
    "b['sentiment'] = df['sentiment']\n",
    "c['sentiment'] = df['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 51) (50000, 51) (50000, 51)\n"
     ]
    }
   ],
   "source": [
    "print(a.shape,b.shape,c.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GET A TRAIN TEST SPLIT (set seed for consistent results)\n",
    "X_1 = a.drop(['sentiment'],axis=1)\n",
    "y_1 = a['sentiment']\n",
    "X_2 = b.drop(['sentiment'],axis=1)\n",
    "y_2 = b['sentiment']\n",
    "X_3 = c.drop(['sentiment'],axis=1)\n",
    "y_3 = c['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATING TRAIN & TEST DF\n",
    "X_1_tr, X_1_te , y_1_tr,y_1_te = train_test_split(X_1, y_1, test_size=0.3, random_state=42)\n",
    "X_2_tr, X_2_te , y_2_tr,y_2_te = train_test_split(X_2, y_2, test_size=0.3, random_state=42)\n",
    "X_3_tr, X_3_te , y_3_tr,y_3_te = train_test_split(X_3, y_3, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Thatoi\\Downloads\\Software\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Thatoi\\Downloads\\Software\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Thatoi\\Downloads\\Software\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of 1gram text classifier  0.6567333333333333\n",
      "Accuracy of 2gramstext classifier  0.6438\n",
      "Accuracy of 3grams text classifier  0.5864\n"
     ]
    }
   ],
   "source": [
    "# SIMPLE LOGISTIC REGRESSION\n",
    "logisticRegr = LogisticRegression()\n",
    "logisticRegr.fit(X_1_tr, y_1_tr)\n",
    "score_1 = logisticRegr.score(X_1_te, y_1_te)\n",
    "print('Accuracy of 1gram text classifier ', score_1)\n",
    "\n",
    "logisticRegr.fit(X_2_tr, y_2_tr)\n",
    "score_2 = logisticRegr.score(X_2_te, y_2_te)\n",
    "print('Accuracy of 2gramstext classifier ', score_2)\n",
    "\n",
    "logisticRegr.fit(X_3_tr, y_3_tr)\n",
    "score_3 = logisticRegr.score(X_3_te, y_3_te)\n",
    "print('Accuracy of 3grams text classifier ', score_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This boils down to data sparsity: As your n-gram length increases, the amount of times you will see any given n-gram will decrease: In the most extreme example, if you have a corpus where the maximum document length is n tokens and you are looking for an m-gram where m=n+1, you will, of course, have no data points at all because it's simply not possible to have a sequence of that length in your data set. The more sparse your data set, the worse you can model it. For this reason, despite that a higher-order n-gram model, in theory, contains more information about a word's context, it cannot easily generalize to other data sets (known as overfitting) because the number of events (i.e. n-grams) it has seen during training becomes progressively less as n increases. On the other hand, a lower-order model lacks contextual information and so may underfit your data.\n",
    "\n",
    "For this reason, if you have a very relatively large amount of token types (i.e. the vocabulary of your text is very rich) but each of these types has a very low frequency, you may get better results with a lower-order n-gram model. Similarly, if your training data set is very small, you may do better with a lower-order n-gram model. However, assuming that you have enough data to avoid over-fitting, you then get better separability of your data with a higher-order model.\n",
    "\n",
    "reference : https://stackoverflow.com/questions/36542993/when-are-uni-grams-more-suitable-than-bi-grams-or-higher-n-grams"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
